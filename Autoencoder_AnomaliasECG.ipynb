{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Autoencoder_AnomaliasECG.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "2LhggqVYpbKD",
        "4lYLeQiZqBU_",
        "Hu3rYuYQrp6z",
        "RiGV9o2W39T0",
        "qY3PpDCT6y5r",
        "WLHf3Qkl76Gv"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xYS6K7IpGCk"
      },
      "source": [
        "# Autoencoders para la detección de anomalías cardiacas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LhggqVYpbKD"
      },
      "source": [
        "## 1- El problema a resolver\n",
        "\n",
        "Detectar la presencia (sujeto *anormal*) o ausencia (sujeto *normal*) de irregularidades en el ritmo cardiaco, a partir de señales electrocardiográficas (ECG):\n",
        "\n",
        "![](https://github.com/adiacla/Autoencoder_Electrocardiograma/blob/main/ecg_normal_anormal.png?raw=true)\n",
        "\n",
        "El problema es que usualmente se cuenta con sets de datos **desbalanceados** con más datos normales que anormales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lYLeQiZqBU_"
      },
      "source": [
        "## 2- El set de datos\n",
        "\n",
        "Se usará el set [ECG5000](https://timeseriesclassification.com/description.php?Dataset=ECG5000) con 7600 datos de entrenamiento y 1900 de prueba.\n",
        "\n",
        "Cada dato contiene un ciclo cardiaco con 140 muestras, y que puede pertenecer a una de 5 categorías:\n",
        "\n",
        "1. Normal\n",
        "2. Anormal: contracción ventricular prematura\n",
        "3. Anormal: contracción supra-ventricular prematura\n",
        "4. Anormal: latido ectópico\n",
        "5. Anormal pero patología desconocida\n",
        "\n",
        "![](https://github.com/adiacla/Autoencoder_Electrocardiograma/blob/main/ejemplo_normales_anormales.png?raw=true)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hu3rYuYQrp6z"
      },
      "source": [
        "## 3- Autoencoders y detección de anomalías\n",
        "\n",
        "El problema del set ECG5000 es que contiene 4427 datos normales y 3173 anormales, es decir está desbalanceado.\n",
        "\n",
        "De hecho para ciertas categorías anormales (2 a 4) se tienen muy pocos datos:\n",
        "\n",
        "| Categoría   | Nro. datos |\n",
        "|-------------|------------|\n",
        "| 1 (normal)  | 4427       |\n",
        "| 2 (anormal) | 2683       |\n",
        "| 3 (anormal) | 149        |\n",
        "| 4 (anormal) | 306        |\n",
        "| 5 (anormal) | 35         |\n",
        "\n",
        "Así que en lugar de un modelo de clasificación convencional (como una Red Neuronal) se usará un [Autoencoder](https://www.codificandobits.com/blog/autoencoders-explicacion-y-tutorial-python/):\n",
        "\n",
        "![](https://github.com/adiacla/Autoencoder_Electrocardiograma/blob/main/autoencoder.png?raw=true)\n",
        "\n",
        "El autoencoder se entrenará **únicamente con datos normales**. Así, al reconstruir un dato anormal **el error será alto**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhx-88b5zFoF"
      },
      "source": [
        "## 4- Implementación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1Zh8TFtzQyO"
      },
      "source": [
        "### 4.1. Preparación de Google Drive y lectura de los sets de entrenamiento y prueba"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rm0D11q3dX5a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKjm_ef9dX5b"
      },
      "source": [
        "ruta = '/gdrive/MyDrive/videos/2021-06-25/'\n",
        "\n",
        "import pandas as pd\n",
        "df_train = pd.read_csv(ruta + 'ECG5000_train.csv')\n",
        "df_test = pd.read_csv(ruta + 'ECG5000_test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jI2mTmbKdrty"
      },
      "source": [
        "print(df_train.shape)\n",
        "print(df_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORzY7-3AvPwZ"
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_zD29g-vUG4"
      },
      "source": [
        "df_train['0'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bKvf6mGzsXO"
      },
      "source": [
        "### 4.2. Sets de entrenamiento y validación"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrLASQllwOC7"
      },
      "source": [
        "# Pandas a arreglos numpy\n",
        "datos_train = df_train.values\n",
        "datos_test = df_test.values\n",
        "\n",
        "# Etiquetas\n",
        "cat_train = datos_train[:,0]\n",
        "cat_test = datos_test[:,0]\n",
        "\n",
        "# Subdivisión por categorías\n",
        "x_train_1 = datos_train[cat_train==1,1:]\n",
        "x_train_2 = datos_train[cat_train==2,1:]\n",
        "x_train_3 = datos_train[cat_train==3,1:]\n",
        "x_train_4 = datos_train[cat_train==4,1:]\n",
        "x_train_5 = datos_train[cat_train==5,1:]\n",
        "\n",
        "x_test_1 = datos_test[cat_test==1,1:]\n",
        "x_test_2 = datos_test[cat_test==2,1:]\n",
        "x_test_3 = datos_test[cat_test==3,1:]\n",
        "x_test_4 = datos_test[cat_test==4,1:]\n",
        "x_test_5 = datos_test[cat_test==5,1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uj8CcBDzdyz-"
      },
      "source": [
        "# Dibujar un dato normal y uno de cada anormal\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "ind = 10\n",
        "normal = x_train_1[ind]\n",
        "anormal_2 = x_train_2[ind]\n",
        "anormal_3 = x_train_3[ind]\n",
        "anormal_4 = x_train_4[ind]\n",
        "anormal_5 = x_train_5[ind]\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.grid()\n",
        "plt.subplot(2,2,1)\n",
        "plt.plot(np.arange(140), normal)\n",
        "plt.plot(np.arange(140), anormal_2, 'r--')\n",
        "plt.subplot(2,2,2)\n",
        "plt.plot(np.arange(140), normal)\n",
        "plt.plot(np.arange(140), anormal_3, 'r--')\n",
        "plt.subplot(2,2,3)\n",
        "plt.plot(np.arange(140), normal)\n",
        "plt.plot(np.arange(140), anormal_4, 'r--')\n",
        "plt.subplot(2,2,4)\n",
        "plt.plot(np.arange(140), normal)\n",
        "plt.plot(np.arange(140), anormal_5, 'r--')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5XL9eA02OPU"
      },
      "source": [
        "### 4.3. Preprocesamiento (normalización)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OgaJsDMhulv"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "min_max_scaler = MinMaxScaler()\n",
        "\n",
        "x_train_1_s = min_max_scaler.fit_transform(x_train_1)\n",
        "print('Mínimo y máximo originales: {:.1f}, {:.1f}'.format(np.min(x_train_1), np.max(x_train_1)))\n",
        "print('Mínimo y máximo normalización: {:.1f}, {:.1f}'.format(np.min(x_train_1_s), np.max(x_train_1_s)))\n",
        "\n",
        "x_test_1_s = min_max_scaler.transform(x_test_1)\n",
        "x_test_2_s = min_max_scaler.transform(x_test_2)\n",
        "x_test_3_s = min_max_scaler.transform(x_test_3)\n",
        "x_test_4_s = min_max_scaler.transform(x_test_4)\n",
        "x_test_5_s = min_max_scaler.transform(x_test_5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDw2-VU823a7"
      },
      "source": [
        "### 4.4. Creación del Autoencoder en TensorFlow/Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJOhS1AEfF9m"
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "np.random.seed(23)\n",
        "\n",
        "dim_entrada = x_train_1_s.shape[1]    #140 muestras/ejemplo\n",
        "entrada = Input(shape=(dim_entrada,))\n",
        "\n",
        "# Encoder\n",
        "encoder = Dense(32, activation='relu')(entrada)\n",
        "encoder = Dense(16, activation='relu')(encoder)\n",
        "encoder = Dense(8, activation='relu')(encoder)\n",
        "\n",
        "# Decoder\n",
        "decoder = Dense(16, activation='relu')(encoder)\n",
        "decoder = Dense(32, activation='relu')(decoder)\n",
        "decoder = Dense(140, activation='sigmoid')(decoder)\n",
        "\n",
        "# Autoencoder = entrada + decoder (que ya contiene el encoder)\n",
        "autoencoder = Model(inputs=entrada, outputs=decoder)\n",
        "\n",
        "autoencoder.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiGV9o2W39T0"
      },
      "source": [
        "### 4.5. Optimización y entrenamiento\n",
        "\n",
        "La pérdida será el error absoluto medio (*Mean Absolute Error*, MAE):\n",
        "\n",
        "$MAE = \\frac{\\sum_{i=1}^{140} |x_i - \\hat{x_i}|}{140}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHMDVi3YgyDP"
      },
      "source": [
        "autoencoder.compile(optimizer='adam', loss='mae')\n",
        "historia = autoencoder.fit(x_train_1_s, x_train_1_s,\n",
        "          epochs=20,\n",
        "          batch_size=512,\n",
        "          validation_data=(x_test_1_s, x_test_1_s),\n",
        "          shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQfvUAQIidoC"
      },
      "source": [
        "plt.plot(historia.history[\"loss\"], label=\"Pérdida set entrenamiento\")\n",
        "plt.plot(historia.history[\"val_loss\"], label=\"Pérdida set prueba\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qY3PpDCT6y5r"
      },
      "source": [
        "## 5- Clasificación"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiwzXShRilD8"
      },
      "source": [
        "# ¿Qué tan bien reconstruye un dato \"normal\" y uno anormal?\n",
        "rec_normal = autoencoder(x_test_1_s).numpy()\n",
        "rec_anormal = autoencoder(x_test_5_s).numpy()\n",
        "\n",
        "dato = 5\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(x_test_1_s[dato],'b')\n",
        "plt.plot(rec_normal[dato],'r')\n",
        "plt.fill_between(np.arange(140), rec_normal[dato], x_test_1_s[dato], color='lightcoral')\n",
        "plt.legend(labels=[\"Original normal\", \"Reconstruction\", \"Error\"])\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(x_test_5_s[dato],'b')\n",
        "plt.plot(rec_anormal[dato],'r')\n",
        "plt.fill_between(np.arange(140), rec_anormal[dato], x_test_5_s[dato], color='lightcoral')\n",
        "plt.legend(labels=[\"Original anormal\", \"Reconstruction\", \"Error\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anB3sJ5CjwJx"
      },
      "source": [
        "# Distribuciones de los errores de construcción\n",
        "# para cada categoría\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "rec_1 = autoencoder.predict(x_test_1_s)\n",
        "rec_2 = autoencoder.predict(x_test_2_s)\n",
        "rec_3 = autoencoder.predict(x_test_3_s)\n",
        "rec_4 = autoencoder.predict(x_test_4_s)\n",
        "rec_5 = autoencoder.predict(x_test_5_s)\n",
        "\n",
        "loss_1 = tf.keras.losses.mae(rec_1, x_test_1_s)\n",
        "loss_2 = tf.keras.losses.mae(rec_2, x_test_2_s)\n",
        "loss_3 = tf.keras.losses.mae(rec_3, x_test_3_s)\n",
        "loss_4 = tf.keras.losses.mae(rec_4, x_test_4_s)\n",
        "loss_5 = tf.keras.losses.mae(rec_5, x_test_5_s)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.hist(loss_1[None,:], bins=100, alpha=0.75, label='normales (1)')\n",
        "plt.hist(loss_2[None,:], bins=100, alpha=0.75, color='#ff521b', label='anormales (2)')\n",
        "plt.hist(loss_3[None,:], bins=100, alpha=0.75, color='#020122', label='anormales (3)')\n",
        "plt.hist(loss_4[None,:], bins=100, alpha=0.75, color='#eefc57', label='anormales (4)')\n",
        "plt.hist(loss_5[None,:], bins=100, alpha=0.75, color='r', label='anormales (5)')\n",
        "plt.xlabel('Pérdidas (MAE)')\n",
        "plt.ylabel('Nro. ejemplos')\n",
        "plt.legend(loc='upper right')\n",
        "plt.vlines(0.08,0,70,'k')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLIBL65fkIy1"
      },
      "source": [
        "umbral = np.mean(loss_1) + np.std(loss_1)\n",
        "print(\"Umbral: \", umbral)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLHf3Qkl76Gv"
      },
      "source": [
        "### 5.1. Sensitividad y especificidad\n",
        "\n",
        "- Verdaderos positivos (TP): anormales que han sido correctamente clasificados como anormales\n",
        "- Falsos negativos (FN): anormales que han sido clasificados erróneamente como normales\n",
        "- Verdaderos negativos (TN): normales que han sido correctamente clasificados como normales\n",
        "- Falsos positivos (FP): normales que han sido clasificados erróneamente como anormales\n",
        "\n",
        "La **sensitividad** mide la proporción de anormales que fueron detectados correctamente como anormales. Una sensitividad del 100% detectará a todos los pacientes enfermos:\n",
        "\n",
        "$sensitividad = \\frac{TP}{TP+FN}$\n",
        "\n",
        "La **especificidad** mide la proporción de normales que fueron detectados correctamente como normales. Una especificidad del 100% detectará a todos los pacientes sanos:\n",
        "\n",
        "$especificidad = \\frac{TN}{TN+FP}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSiVwHm3gEwp"
      },
      "source": [
        "# Calcular predicciones individuales\n",
        "def predecir(modelo, datos, umbral):\n",
        "  reconstrucciones = modelo(datos)\n",
        "  perdida = tf.keras.losses.mae(reconstrucciones, datos)\n",
        "  return tf.math.less(perdida, umbral)\n",
        "\n",
        "def calcular_sensitividad(prediccion, titulo):\n",
        "  TP = np.count_nonzero(~prediccion)\n",
        "  FN = np.count_nonzero(prediccion)\n",
        "  sen = 100*(TP/(TP+FN))\n",
        "\n",
        "  print(titulo + ': {:.1f}%'.format(sen))\n",
        "\n",
        "def calcular_especificidad(prediccion, titulo):\n",
        "  TN = np.count_nonzero(prediccion)\n",
        "  FP = np.count_nonzero(~prediccion)\n",
        "  esp = 100*(TN/(TN+FP))\n",
        "\n",
        "  print(titulo + ': {:.1f}%'.format(esp))\n",
        "  return esp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCFpj0gC__U2"
      },
      "source": [
        "# Predicciones\n",
        "pred_1 = predecir(autoencoder, x_test_1_s, umbral)\n",
        "pred_2 = predecir(autoencoder, x_test_2_s, umbral)\n",
        "pred_3 = predecir(autoencoder, x_test_3_s, umbral)\n",
        "pred_4 = predecir(autoencoder, x_test_4_s, umbral)\n",
        "pred_5 = predecir(autoencoder, x_test_5_s, umbral)\n",
        "\n",
        "# Pred1: especificidad\n",
        "esp_1 = calcular_especificidad(pred_1,'Especificidad (cat. 1, normales)')\n",
        "\n",
        "# Pred 2 a 5: sensitividad\n",
        "sen_2 = calcular_sensitividad(pred_2,'Sensitividad (cat. 2, anormales)')\n",
        "sen_3 = calcular_sensitividad(pred_3,'Sensitividad (cat. 3, anormales)')\n",
        "sen_4 = calcular_sensitividad(pred_4,'Sensitividad (cat. 4, anormales)')\n",
        "sen_5 = calcular_sensitividad(pred_3,'Sensitividad (cat. 5, anormales)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNI8cX6x2BLJ"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}